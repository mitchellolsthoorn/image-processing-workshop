{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Authored by Dr. Mitchell Olsthoorn and Dr. Annibale Panichella\n",
    "\n",
    "# Requirements\n",
    "\n",
    "### Step 1: We first import the required libraries"
   ],
   "id": "1d9affef1af7af31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import cv2 # Python OpenCV (Open Computer Vision) library\n",
    "import numpy as np # Numpy library for working with arrays and matrices\n",
    "import matplotlib.pyplot as plt # Matplot library for displaying images\n",
    "from IPython.display import display # IPython library for interactive controls\n",
    "import ipywidgets as widgets # IPython library for interactive controls"
   ],
   "id": "3f18ef497513dc92",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 2: Define a function for displaying OpenCV images within Jupyter notebooks",
   "id": "4a10642a2b37cdab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def display_image(image, title, size=(15, 15), grayscale=False):\n",
    "    plt.figure(figsize=size)\n",
    "    \n",
    "    if grayscale:\n",
    "        plt.imshow(image, cmap='gray') # Display the image using matplotlib in grayscale\n",
    "    else:\n",
    "        plt.imshow(image) # Display the image using matplotlib in color\n",
    "        \n",
    "    plt.title(title) # Set the title of the image\n",
    "    plt.axis('off') # Remove the axis lines and labels for a cleaner display\n",
    "    plt.show() # Show the image in a window\n",
    "\n",
    "def compare_images(images, size=(15, 15), grayscale=False):\n",
    "    plt.figure(figsize=size)\n",
    "    \n",
    "    for index, image in enumerate(images):\n",
    "        plt.subplot(1, len(images), index + 1)\n",
    "        \n",
    "        if grayscale:\n",
    "            plt.imshow(image[0], cmap='gray') # Display the image using matplotlib in grayscale\n",
    "        else:\n",
    "            plt.imshow(image[0]) # Display the image using matplotlib in color\n",
    "            \n",
    "        plt.title(image[1])\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ],
   "id": "713a0ef18c8a51cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 3: Load an image into the program",
   "id": "cd38fc0a0a1f0b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "files = [\n",
    "    'images/tiger.jpeg',\n",
    "    'images/moon.jpg',\n",
    "    'images/monalisa.png',\n",
    "    'images/salt_pepper.jpg'\n",
    "]\n",
    "\n",
    "images = []\n",
    "\n",
    "for file in files:\n",
    "    image_bgr = cv2.imread(file)\n",
    "    image = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB) # Convert the image from BGR (used by OpenCV) to RGB (used by matplotlib)\n",
    "    images.append(image)\n",
    "\n",
    "image = images[0]\n",
    "\n",
    "display_image(image, \"Original Image\")"
   ],
   "id": "c184efcfb438418a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 4: Properties of Images",
   "id": "90a044e640f2fc87"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### How is an image structured in code?",
   "id": "a1032053c9eb81a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(image.shape)\n",
    "\n",
    "print(image[200, 550])"
   ],
   "id": "bfabe633af0d6c7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### What does each channel look like?",
   "id": "1e7985d39cf24a19"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extract the individual color channels \n",
    "r, g, b = cv2.split(image)\n",
    "\n",
    "height, width = image.shape[:2]\n",
    "blank = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "R = cv2.merge([r, blank, blank])\n",
    "G = cv2.merge([blank, g, blank])\n",
    "B = cv2.merge([blank, blank, b])\n",
    "\n",
    "# Display the individual channels in their respective colors\n",
    "compare_images([\n",
    "    [R, 'Red Channel'],\n",
    "    [G, 'Green Channel'],\n",
    "    [B, 'Blue Channel'],\n",
    "])"
   ],
   "id": "ba41e7f35a635a1e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(r[200, 550])\n",
    "\n",
    "print(np.matrix(r))"
   ],
   "id": "694335c7b4251f12",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Convert to grayscale",
   "id": "63fb4b3fba0b772"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Show full grayscale\n",
    "image_grayscale = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "display_image(image_grayscale, \"Grayscale Image\", grayscale=True)\n",
    "\n",
    "print(image_grayscale[200, 550])"
   ],
   "id": "b42f5e30df9c662f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Quantization: how many bits per pixel?",
   "id": "7024bfc040c60073"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def reduce_bit_depth(image, bits):\n",
    "    # Calculate the factor by which the values are reduced\n",
    "    factor = 256 // (2 ** bits)  # E.g., for 4-bit depth: 256 // 16 = 16\n",
    "    \n",
    "    reduced_image = image // factor * factor # Reduce and scale back pixel values\n",
    "    return reduced_image"
   ],
   "id": "3e90dc5e2d72357c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image_8_bit = image_grayscale\n",
    "image_7_bit = reduce_bit_depth(image_8_bit, 7)\n",
    "image_6_bit = reduce_bit_depth(image_8_bit, 6)\n",
    "image_5_bit = reduce_bit_depth(image_8_bit, 5)\n",
    "image_4_bit = reduce_bit_depth(image_8_bit, 4)\n",
    "image_3_bit = reduce_bit_depth(image_8_bit, 3)\n",
    "image_2_bit = reduce_bit_depth(image_8_bit, 2)\n",
    "image_1_bit = reduce_bit_depth(image_8_bit, 1)\n",
    "\n",
    "display_image(image_8_bit, \"Original Image (8-bit) - 256 values\", grayscale=True)\n",
    "display_image(image_7_bit, \"Image (7-bit) - 128 values\", grayscale=True)\n",
    "display_image(image_6_bit, \"Image (6-bit) - 64 values\", grayscale=True)\n",
    "display_image(image_5_bit, \"Image (5-bit) - 32 values\", grayscale=True)\n",
    "display_image(image_4_bit, \"Image (4-bit) - 16 values\", grayscale=True)\n",
    "display_image(image_3_bit, \"Image (3-bit) - 8 values\", grayscale=True)\n",
    "display_image(image_2_bit, \"Image (2-bit) - 4 values\", grayscale=True)\n",
    "display_image(image_1_bit, \"Image (1-bit) - 2 values\", grayscale=True)"
   ],
   "id": "6d8bd671a77939d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_grayscale_gradient(width, height):\n",
    "    # Create a 2D array with values ranging from 255 (white) to 0 (black)\n",
    "    gradient = np.linspace(255, 0, width).astype(np.uint8)  # Generate a row with a gradient from white to black\n",
    "    gradient_image = np.tile(gradient, (height, 1))  # Repeat the row to create a full image\n",
    "    return gradient_image"
   ],
   "id": "b59b562fd6a87622",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "grayscale_gradiant = create_grayscale_gradient(250, 10)\n",
    "\n",
    "grayscale_gradiant_8_bit = grayscale_gradiant\n",
    "grayscale_gradiant_7_bit = reduce_bit_depth(grayscale_gradiant, 7)\n",
    "grayscale_gradiant_6_bit = reduce_bit_depth(grayscale_gradiant, 6)\n",
    "grayscale_gradiant_5_bit = reduce_bit_depth(grayscale_gradiant, 5)\n",
    "grayscale_gradiant_4_bit = reduce_bit_depth(grayscale_gradiant, 4)\n",
    "grayscale_gradiant_3_bit = reduce_bit_depth(grayscale_gradiant, 3)\n",
    "grayscale_gradiant_2_bit = reduce_bit_depth(grayscale_gradiant, 2)\n",
    "grayscale_gradiant_1_bit = reduce_bit_depth(grayscale_gradiant, 1)\n",
    "\n",
    "display_image(grayscale_gradiant_8_bit, \"Grayscale Range (8-bit) - 256 values\", grayscale=True)\n",
    "display_image(grayscale_gradiant_7_bit, \"Grayscale Range (7-bit) - 128 values\", grayscale=True)\n",
    "display_image(grayscale_gradiant_6_bit, \"Grayscale Range (6-bit) - 64 values\", grayscale=True)\n",
    "display_image(grayscale_gradiant_5_bit, \"Grayscale Range (5-bit) - 32 values\", grayscale=True)\n",
    "display_image(grayscale_gradiant_4_bit, \"Grayscale Range (4-bit) - 16 values\", grayscale=True)\n",
    "display_image(grayscale_gradiant_3_bit, \"Grayscale Range (3-bit) - 8 values\", grayscale=True)\n",
    "display_image(grayscale_gradiant_2_bit, \"Grayscale Range (2-bit) - 4 values\", grayscale=True)\n",
    "display_image(grayscale_gradiant_1_bit, \"Grayscale Range (1-bit) - 2 values\", grayscale=True)"
   ],
   "id": "13ddd87c4678713",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Image Filtering\n",
    "\n",
    "Image filtering is a technique used to process and manipulate images by applying a mathematical operation to the pixel values. The goal is to enhance certain features or reduce unwanted elements, such as noise, in an image. Filters can either sharpen, blur, detect edges, or transform the image in various ways, depending on the filter used.\n",
    "\n",
    "## Kernal\n",
    "\n",
    "In the context of image filtering, a kernel (also called a filter or convolution matrix) is a small matrix used to apply effects like blurring, sharpening, edge detection, and more to an image. The kernel is usually much smaller than the image itself (often 3x3, 5x5, or similar), and it is applied to every pixel in the image to compute a new pixel value based on the surrounding ones.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. Kernel Matrix: The kernel is a matrix of numbers that defines the transformation to apply. Each number in the kernel represents how much the surrounding pixels contribute to the final value of the current pixel.\n",
    "\n",
    "2. Convolution Operation: The kernel moves (slides) across the image pixel by pixel, and at each position, it multiplies the values in the kernel by the corresponding pixel values in the image (element-wise multiplication). The results are then summed up, and the central pixel is replaced with the new value.\n",
    "\n",
    "3. Effect of Kernel Size and Values:\n",
    "\n",
    "- Blurring: A kernel with equal values (like 1/9 for a 3x3 kernel) will average out pixel values, creating a blur effect.\n",
    "- Edge Detection: A kernel like [[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]] emphasizes differences between adjacent pixels, detecting edges.\n",
    "- Sharpening: Kernels with a higher central value and negative surrounding values enhance contrast around edges, sharpening the image."
   ],
   "id": "f71666f1c11fc62a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image_identity = images[0]\n",
    "\n",
    "# Apply identity kernel\n",
    "kernel_identity = np.array([[0, 0, 0],\n",
    "                            [0, 1, 0],\n",
    "                            [0, 0, 0]])\n",
    "\n",
    "image_identity_filtered = cv2.filter2D(src=image_identity, ddepth=-1, kernel=kernel_identity)\n",
    "\n",
    "compare_images([\n",
    "    [image_identity, 'Original Image'],\n",
    "    [image_identity_filtered, 'Identity Image'],\n",
    "])"
   ],
   "id": "620018fe536528f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## OpenCV Blurring Techniques:",
   "id": "6663d408e1595190"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Averaging\n",
    "\n",
    "An averaging filter (also known as a mean filter) is one of the simplest types of image filtering techniques. Its primary purpose is to smooth an image by reducing noise and minor details, which is achieved by replacing each pixel's value with the average of its neighboring pixel values. This results in a blurred or softened version of the image, as sharp transitions between pixel values (like edges or noise) are smoothed out."
   ],
   "id": "a8b059f0f78eac5a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image_averaging = images[0]\n",
    "\n",
    "# Apply blurring kernel\n",
    "# kernel_averaging = np.ones((5, 5), np.float32) / 25\n",
    "# image_averaging_filtered = cv2.filter2D(src=image_averaging, ddepth=-1, kernel=kernel_averaging)\n",
    "\n",
    "# OR\n",
    "\n",
    "image_averaging_filtered = cv2.blur(src=image_averaging, ksize=(5,5)) # Using the blur function to blur an image where ksize is the kernel size\n",
    "\n",
    "compare_images([\n",
    "    [image_averaging, 'Original Image'],\n",
    "    [image_averaging_filtered, 'Averaging Filtered Image'],\n",
    "])"
   ],
   "id": "b9cea75d40a89278",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Gaussian Blurring\n",
    "\n",
    "Gaussian filtering is a linear, smoothing filter used to blur an image and reduce noise. It is based on the Gaussian function, which produces a bell-shaped curve, making the filter particularly useful for removing high-frequency components (like noise) while maintaining the overall structure of the image. Unlike simple averaging filters, Gaussian filtering applies different weights to pixels, giving more importance to pixels near the center of the kernel."
   ],
   "id": "3deffb7c9a1c7fd1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image_gaussian = images[0]\n",
    "\n",
    "image_gaussian_filtered = cv2.GaussianBlur(src=image_gaussian, ksize=(5,5), sigmaX=0, sigmaY=0) # Sigma values determine the variance\n",
    "\n",
    "compare_images([\n",
    "    [image_gaussian, 'Original Image'],\n",
    "    [image_gaussian_filtered, 'Gaussian Filtered Image'],\n",
    "])"
   ],
   "id": "2c82fd9094a752c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Median Blurring\n",
    "\n",
    "Median filtering is a simple, non-linear filtering technique primarily used to reduce noise in an image while preserving edges. Itâ€™s particularly effective at removing impulse noise (also called salt-and-pepper noise), which consists of random bright and dark pixels that corrupt an image."
   ],
   "id": "b5ec5898f4fce3ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image_median = images[2]\n",
    "\n",
    "image_median_filtered = cv2.medianBlur(src=image_median, ksize=5)\n",
    "\n",
    "compare_images([\n",
    "    [image_median, 'Original Image'],\n",
    "    [image_median_filtered, 'Median Filtered Image'],\n",
    "])"
   ],
   "id": "49445f6f5f8a6ed1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Bilateral Filtering\n",
    "\n",
    "Bilateral filtering is a non-linear, edge-preserving, and noise-reducing image filtering technique. Unlike traditional filters that blur edges, bilateral filtering smooths the image while keeping the sharpness of edges intact."
   ],
   "id": "198eb568bed0c7fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image_bilateral = images[1]\n",
    "\n",
    "image_bilateral_filtered = cv2.bilateralFilter(src=image_bilateral, d=5, sigmaColor=75, sigmaSpace=75)\n",
    "\n",
    "compare_images([\n",
    "    [image_bilateral, 'Original Image'],\n",
    "    [image_bilateral_filtered, 'Median Filtered Image'],\n",
    "])"
   ],
   "id": "6698660baed85671",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Edge and Contour Detection\n",
    "\n",
    "Edge and contour detection are crucial techniques in image processing and computer vision for identifying the boundaries or outlines of objects within an image. These methods are used to simplify the image by highlighting important structural features, which are often essential for further analysis, such as object recognition, segmentation, and feature extraction.\n",
    "\n",
    "Edge detection involves identifying points in an image where the intensity changes abruptly, indicating the boundaries between different objects or regions. These intensity changes usually occur where objects' colors, brightness, or textures differ, and they form edges, which are one-dimensional curves that represent the boundaries.\n",
    "\n",
    "Contour detection is the process of identifying and tracing the contours or boundaries of objects within an image. While edges are local, pixel-level changes in intensity, contours refer to the continuous curves or shapes that follow these edges, representing the outline of an object."
   ],
   "id": "7296fdff0da06b05"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image_difference = images[0]\n",
    "\n",
    "kernel_difference1 = np.array([[-1, 0, 1],\n",
    "                               [-1, 0, 1],\n",
    "                               [-1, 0, 1]])\n",
    "\n",
    "kernel_difference2 = np.array([[ 0, -1,  0],\n",
    "                               [-1,  4, -1],\n",
    "                               [ 0, -1,  0]])\n",
    "\n",
    "kernel_difference3 = np.array([[-1, -1, -1],\n",
    "                               [-1,  8, -1],\n",
    "                               [-1, -1, -1]])\n",
    "\n",
    "kernel_difference4 = np.array([[ 1,  1,  1],\n",
    "                               [ 0,  0,  0],\n",
    "                               [-1, -1, -1]])\n",
    "\n",
    "image_difference_filtered = cv2.filter2D(src=image_difference, ddepth=-1, kernel=kernel_difference4)\n",
    "\n",
    "# image_difference_filtered1 = cv2.filter2D(src=image_difference, ddepth=-1, kernel=kernel_difference1)\n",
    "# image_difference_filtered2 = cv2.filter2D(src=image_difference, ddepth=-1, kernel=kernel_difference4)\n",
    "# image_difference_filtered = cv2.add(image_difference_filtered1, image_difference_filtered2)\n",
    "\n",
    "compare_images([\n",
    "    [image_difference, 'Original Image'],\n",
    "    [image_difference_filtered, 'Filtered Image'],\n",
    "])"
   ],
   "id": "33ffb96c848358b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Sharpening Images\n",
    "\n",
    "We can use the edges detected by different techniques to sharpen an image by making the edges of objects within the image more pronounced. This process generally entails detecting the edges in the image and then amplifying the contrast around these edges to improve the overall sharpness."
   ],
   "id": "999357120efc3de2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image_difference_sharpened = cv2.add(image_difference, image_difference_filtered)\n",
    "\n",
    "compare_images([\n",
    "    [image_difference, 'Original Image'],\n",
    "    [image_difference_sharpened, 'Sharpened Image'],\n",
    "])"
   ],
   "id": "355f18b52fa4f438",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Canny Edge Detection",
   "id": "6554b7815e47183c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image_canny = images[0]\n",
    "image_canny = cv2.cvtColor(image_canny, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "image_canny_edges = cv2.Canny(image_canny, 100, 200)\n",
    "\n",
    "compare_images([\n",
    "    [image_canny, 'Original Image'],\n",
    "    [image_canny_edges, 'Image Edges'],\n",
    "], grayscale=True)"
   ],
   "id": "d56e160ed0c7e6c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def canny_edge_detection(hysteresis_low, hysteresis_high):\n",
    "    image_canny = images[0]\n",
    "    image_canny = cv2.cvtColor(image_canny, cv2.COLOR_RGB2GRAY)\n",
    "    image_canny_edges = cv2.Canny(image_canny, hysteresis_low, hysteresis_high)\n",
    "\n",
    "    compare_images([\n",
    "        [image_canny, 'Original Image'],\n",
    "        [image_canny_edges, 'Image Edges'],\n",
    "    ], grayscale=True)\n",
    "\n",
    "\n",
    "\n",
    "# Create an IntSlider to adjust hysteresis thresholds\n",
    "slider_low = widgets.IntSlider(value=100, min=1, max=500, step=2, description='Low')\n",
    "slider_high = widgets.IntSlider(value=200, min=1, max=500, step=2, description='High')\n",
    "\n",
    "# Use interactive display to update the image as the slider is adjusted\n",
    "widgets.interact(canny_edge_detection, hysteresis_low=slider_low, hysteresis_high=slider_high)"
   ],
   "id": "54aa573465407b74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Contour Detection",
   "id": "8e49df693dc77b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image_contour = images[0]\n",
    "image_contour = cv2.cvtColor(image_contour, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "image_contour_ret, image_contour_thresh = cv2.threshold(image_contour, 150, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "compare_images([\n",
    "    [image_contour, 'Original Image'],\n",
    "    [image_contour_thresh, 'Segmented Image'],\n",
    "], grayscale=True)"
   ],
   "id": "59966d063a27106f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image_contour_contours, image_contour_hierarchy = cv2.findContours(image=image_contour_thresh, mode=cv2.RETR_TREE, method=cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "# Draw contours on the original image\n",
    "image_contour_copy = image_contour.copy()\n",
    "cv2.drawContours(image=image_contour_copy, contours=image_contour_contours, contourIdx=-1, color=(0, 255, 0), thickness=2, lineType=cv2.LINE_AA)\n",
    "\n",
    "compare_images([\n",
    "    [image_contour, 'Original Image'],\n",
    "    [image_contour_copy, 'Contoured Image'],\n",
    "], grayscale=True)"
   ],
   "id": "d7be7650b7e40ba9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Advanced Examples",
   "id": "84ab30d953750e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Lane Detection (Adapted from https://www.geeksforgeeks.org/opencv-real-time-road-lane-detection/)",
   "id": "a5e7191384aa707a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def process_frame(image):\n",
    "    # Convert the RGB image to Gray scale\n",
    "    grayscale = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Applying gaussian blur to remove noise from the frames\n",
    "    blur = cv2.GaussianBlur(grayscale, (5, 5), 0)\n",
    "\n",
    "    # Applying canny edge detection and save edges in a variable\n",
    "    edges = cv2.Canny(blur, 50, 150)\n",
    "\n",
    "    # Since we are getting too many edges from our image, we apply a mask polygon to only focus on the road\n",
    "    mask, region = region_selection(edges)\n",
    "\n",
    "    # Applying hough transform to get straight lines from our image and find the lane lines\n",
    "    hough = cv2.HoughLinesP(region, rho = 1, theta = (np.pi/180), threshold = 20,\n",
    "                                   minLineLength = 20, maxLineGap = 500)\n",
    "\n",
    "    # Lastly we draw the lines on our resulting frame and return it as output\n",
    "    if hough is not None:\n",
    "        result = draw_lane_lines(image, lane_lines(image, hough))\n",
    "        return result\n",
    "\n",
    "    return image\n",
    "\n",
    "def region_selection(image):\n",
    "    # Create an array of the same size as of the input image\n",
    "    mask = np.zeros_like(image)\n",
    "\n",
    "    # Creating a polygon to focus only on the road in the picture\n",
    "    # This polygon was created in accordance to how the camera was placed\n",
    "    rows, cols = image.shape[:2]\n",
    "    bottom_left = [cols * 0.1, rows * 0.95]\n",
    "    top_left\t = [cols * 0.4, rows * 0.6]\n",
    "    bottom_right = [cols * 0.9, rows * 0.95]\n",
    "    top_right = [cols * 0.6, rows * 0.6]\n",
    "\n",
    "\n",
    "    # bottom_left = [cols * 0.1, rows * 0.70]\n",
    "    # bottom_right = [cols * 0.9, rows * 0.70]\n",
    "    # top_left\t = [cols * 0.1, rows * 0.2]\n",
    "    # top_right = [cols * 0.9, rows * 0.2]\n",
    "    vertices = np.array([[bottom_left, top_left, top_right, bottom_right]], dtype=np.int32)\n",
    "\n",
    "    # Filling the polygon with white color and generating the final mask\n",
    "    cv2.fillPoly(mask, vertices, 255)\n",
    "\n",
    "    # Performing Bitwise AND on the input image and mask to get only the edges on the road\n",
    "    masked_image = cv2.bitwise_and(image, mask)\n",
    "\n",
    "    return mask, masked_image\n",
    "\n",
    "def draw_lane_lines(image, lines, color=[255, 0, 0], thickness=12):\n",
    "    \"\"\"\n",
    "    Draw lines onto the input image.\n",
    "        Parameters:\n",
    "            image: The input test image (video frame in our case).\n",
    "            lines: The output lines from Hough Transform.\n",
    "            color (Default = red): Line color.\n",
    "            thickness (Default = 12): Line thickness.\n",
    "    \"\"\"\n",
    "\n",
    "    line_image = np.zeros_like(image)\n",
    "    for line in lines:\n",
    "        if line is not None:\n",
    "            cv2.line(line_image, *line, color, thickness)\n",
    "\n",
    "    return cv2.addWeighted(image, 1.0, line_image, 1.0, 0.0)\n",
    "\n",
    "def lane_lines(image, lines):\n",
    "    \"\"\"\n",
    "    Create full lenght lines from pixel points.\n",
    "        Parameters:\n",
    "            image: The input test image.\n",
    "            lines: The output lines from Hough Transform.\n",
    "    \"\"\"\n",
    "\n",
    "    left_lane, right_lane = average_slope_intercept(lines)\n",
    "    y1 = image.shape[0]\n",
    "    y2 = y1 * 0.6\n",
    "    left_line = pixel_points(y1, y2, left_lane)\n",
    "    right_line = pixel_points(y1, y2, right_lane)\n",
    "\n",
    "    return left_line, right_line\n",
    "    \n",
    "def average_slope_intercept(lines):\n",
    "    \"\"\"\n",
    "    Find the slope and intercept of the left and right lanes of each image.\n",
    "    Parameters:\n",
    "        lines: output from Hough Transform\n",
    "    \"\"\"\n",
    "    \n",
    "    left_lines = [] #(slope, intercept)\n",
    "    left_weights = [] #(length,)\n",
    "    right_lines = [] #(slope, intercept)\n",
    "    right_weights = [] #(length,)\n",
    "\n",
    "    for line in lines:\n",
    "        for x1, y1, x2, y2 in line:\n",
    "            if x1 == x2:\n",
    "                continue\n",
    "                \n",
    "            # calculating slope of a line\n",
    "            slope = (y2 - y1) / (x2 - x1)\n",
    "            \n",
    "            # calculating intercept of a line\n",
    "            intercept = y1 - (slope * x1)\n",
    "            \n",
    "            # calculating length of a line\n",
    "            length = np.sqrt(((y2 - y1) ** 2) + ((x2 - x1) ** 2))\n",
    "            \n",
    "            # slope of left lane is negative and for right lane slope is positive\n",
    "            if slope < 0:\n",
    "                left_lines.append((slope, intercept))\n",
    "                left_weights.append((length))\n",
    "            else:\n",
    "                right_lines.append((slope, intercept))\n",
    "                right_weights.append((length))\n",
    "                \n",
    "    left_lane = np.dot(left_weights, left_lines) / np.sum(left_weights) if len(left_weights) > 0 else None\n",
    "    right_lane = np.dot(right_weights, right_lines) / np.sum(right_weights) if len(right_weights) > 0 else None\n",
    "    return left_lane, right_lane\n",
    "\n",
    "def pixel_points(y1, y2, line):\n",
    "    \"\"\"\n",
    "    Converts the slope and intercept of each line into pixel points.\n",
    "        Parameters:\n",
    "            y1: y-value of the line's starting point.\n",
    "            y2: y-value of the line's end point.\n",
    "            line: The slope and intercept of the line.\n",
    "    \"\"\"\n",
    "    if line is None:\n",
    "        return None\n",
    "    \n",
    "    slope, intercept = line\n",
    "    if slope == 0:\n",
    "        return None\n",
    "\n",
    "    x1 = int((y1 - intercept)/slope)\n",
    "    x2 = int((y2 - intercept)/slope)\n",
    "    y1 = int(y1)\n",
    "    y2 = int(y2)\n",
    "    \n",
    "    return ((x1, y1), (x2, y2))"
   ],
   "id": "84fa8a5beaa8dfcd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image_lane_detection = cv2.imread('images/road.png')\n",
    "image_lane_detection_lines = process_frame(image_lane_detection)\n",
    "\n",
    "compare_images([\n",
    "    [cv2.cvtColor(image_lane_detection, cv2.COLOR_BGR2RGB), 'Original Image'],\n",
    "    [cv2.cvtColor(image_lane_detection_lines, cv2.COLOR_BGR2RGB), 'Contoured Image'],\n",
    "])"
   ],
   "id": "1ad6d36830c56213",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert the RGB image to Gray scale\n",
    "image_lane_detection_grayscale = cv2.cvtColor(image_lane_detection, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Applying gaussian blur to remove noise from the frames\n",
    "image_lane_detection_blur = cv2.GaussianBlur(image_lane_detection_grayscale, (5, 5), 0)\n",
    "\n",
    "# Applying canny edge detection and save edges in a variable\n",
    "image_lane_detection_edges = cv2.Canny(image_lane_detection_blur, 50, 150)\n",
    "\n",
    "# Since we are getting too many edges from our image, we apply a mask polygon to only focus on the road\n",
    "image_lane_detection_mask, image_lane_detection_region = region_selection(image_lane_detection_edges)\n",
    "\n",
    "compare_images([\n",
    "    [image_lane_detection_blur, 'Blurred Image'],\n",
    "    [image_lane_detection_edges, 'Edges'],\n",
    "    [image_lane_detection_mask, 'Mask'],\n",
    "    [image_lane_detection_region, 'Masked Edges'],\n",
    "], grayscale=True)"
   ],
   "id": "727a22cd9e55ce4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from moviepy import editor\n",
    "\n",
    "def process_video(test_video, output_video):\n",
    "    # read the video file using VideoFileClip without audio\n",
    "    input_video = editor.VideoFileClip(test_video, audio=False)\n",
    "\n",
    "    # apply the function \"frame_processor\" to each frame of the video\n",
    "    # will give more detail about \"frame_processor\" in further steps\n",
    "    # \"processed\" stores the output video\n",
    "    processed = input_video.fl_image(process_frame)\n",
    "\n",
    "    # save the output video stream to an mp4 file\n",
    "    processed.write_videofile(output_video, audio=False)"
   ],
   "id": "b1da4dccc069c82e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "process_video('images/road.mp4', 'images/road_lines.mp4')",
   "id": "bfebaadbda57e7e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Live Edge Detection (Adapted from https://www.geeksforgeeks.org/real-time-edge-detection-using-opencv-python/)",
   "id": "135768181873a047"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "counter = 0\n",
    "\n",
    "while counter < 1:\n",
    "    # Read a frame from the webcam \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print('Image not captured')\n",
    "        break\n",
    "\n",
    "    # Convert frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply Gaussian blur to reduce noise and smoothen edges \n",
    "    blurred = cv2.GaussianBlur(src=gray, ksize=(3, 5), sigmaX=0.5)\n",
    "\n",
    "    # Perform Canny edge detection \n",
    "    edges = cv2.Canny(blurred, 70, 135)\n",
    "\n",
    "    # Display the original frame and the edge-detected frame\n",
    "    compare_images([\n",
    "        [cv2.cvtColor(blurred, cv2.COLOR_BGR2RGB), 'Blurred Image'],\n",
    "        [cv2.cvtColor(edges, cv2.COLOR_BGR2RGB), 'Edges'],\n",
    "    ])\n",
    "\n",
    "    counter += 1"
   ],
   "id": "ce6557c508191405",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Live Face Detection (Adapted from https://medium.com/pythons-gurus/what-is-the-best-face-detector-ab650d8c1225)",
   "id": "2f9cd739e62a188c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "yunet = cv2.FaceDetectorYN_create(model = 'images/face_detection_yunet_2023mar.onnx',\n",
    "                                  config = \"\",\n",
    "                                  input_size = (300, 300),\n",
    "                                  score_threshold=0.5)\n",
    "\n",
    "def detect_faces(image):\n",
    "    yunet.setInputSize((image.shape[1], image.shape[0]))\n",
    "    img_size = (image.shape[1], image.shape[0])\n",
    "    _, faces = yunet.detect(image)\n",
    "    \n",
    "    if faces is None:\n",
    "        return None\n",
    "    else:\n",
    "        return parse_predictions(image, faces, img_size)\n",
    "\n",
    "def parse_predictions(image, faces, img_size):\n",
    "    data = []\n",
    "    for index, face in enumerate(list(faces)):\n",
    "        x1, y1, x2, y2 = list(map(int, face[:4]))\n",
    "        landmarks = list(map(int, face[4:len(face)-1]))\n",
    "        landmarks = np.array_split(landmarks, len(landmarks) / 2)\n",
    "        positions = ['left_eye', 'right_eye', 'nose', 'right_mouth', 'left_mouth']\n",
    "        landmarks = {positions[num]: x.tolist() for num, x in enumerate(landmarks)}\n",
    "        confidence = face[-1]\n",
    "        datum = {'x1': x1,\n",
    "                 'y1': y1,\n",
    "                 'x2': x2,\n",
    "                 'y2': y2,\n",
    "                 'face_num': index,\n",
    "                 'landmarks': landmarks,\n",
    "                 'confidence': confidence,\n",
    "                 'model': 'yunet'}\n",
    "        d = scale_coords(image, datum, img_size)\n",
    "        data.append(d)\n",
    "    return data\n",
    "\n",
    "def scale_coords(image, prediction, img_size):\n",
    "    ih, iw = image.shape[:2]\n",
    "    rw, rh = img_size\n",
    "    a = np.array([\n",
    "        (prediction['x1'], prediction['y1']),\n",
    "        (prediction['x1'] + prediction['x2'], prediction['y1'] + prediction['y2'])\n",
    "    ])\n",
    "    b = np.array([iw/rw, ih/rh])\n",
    "    c = a * b\n",
    "    prediction['img_width'] = iw\n",
    "    prediction['img_height'] = ih\n",
    "    prediction['x1'] = int(c[0,0].round())\n",
    "    prediction['x2'] = int(c[1,0].round())\n",
    "    prediction['y1'] = int(c[0,1].round())\n",
    "    prediction['y2'] = int(c[1,1].round())\n",
    "    prediction['face_width'] = (c[1,0] - c[0,0])\n",
    "    prediction['face_height'] = (c[1,1] - c[0,1])\n",
    "    prediction['area'] = prediction['face_width'] * prediction['face_height']\n",
    "    prediction['pct_of_frame'] = prediction['area']/(prediction['img_width'] * prediction['img_height'])\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "def draw_faces(image, faces, draw_landmarks=False, show_confidence=False):\n",
    "    for face in faces:\n",
    "        color = (0, 0, 255)\n",
    "        thickness = 2\n",
    "        cv2.rectangle(image, (face['x1'], face['y1']), (face['x2'], face['y2']), color, thickness, cv2.LINE_AA)\n",
    "\n",
    "        if draw_landmarks:\n",
    "            landmarks = face['landmarks']\n",
    "            for landmark in landmarks:\n",
    "                radius = 5\n",
    "                thickness = -1\n",
    "                cv2.circle(image, landmarks[landmark], radius, color, thickness)\n",
    "\n",
    "        if show_confidence:\n",
    "            confidence = face['confidence']\n",
    "            confidence = \"{:.2f}\".format(confidence)\n",
    "            position = (face['x1'], face['y1'] - 10)\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            scale = 0.5\n",
    "            thickness = 2\n",
    "            cv2.putText(image, confidence, position, font, scale, color, thickness, cv2.LINE_AA)\n",
    "    return image\n"
   ],
   "id": "66c1d0b2c83dd06b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "counter = 0\n",
    "\n",
    "while counter < 1:\n",
    "    # Read a frame from the webcam \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print('Image not captured')\n",
    "        break\n",
    "\n",
    "    # Convert frame to grayscale\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Detect faces\n",
    "    faces = detect_faces(frame_rgb)\n",
    "\n",
    "    if faces:\n",
    "        print(len(faces))\n",
    "        draw_faces(frame_rgb, faces, True, True)\n",
    "\n",
    "    display_image(frame_rgb, \"Face\")\n",
    "\n",
    "    counter += 1"
   ],
   "id": "85af2cb3424470d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
